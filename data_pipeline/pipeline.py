"""
ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë©”ì¸ í´ëž˜ìŠ¤

4ë‹¨ê³„ ETL íŒŒì´í”„ë¼ì¸:
1. Extract: ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ìž¥
2. Transform: íŒŒì‹± ë° ì •ê·œí™”
3. Validate: ê¸°ìˆ ì /ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ì¦
4. Load: ê²€ì¦ëœ ë°ì´í„° ì €ìž¥
"""

from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from loguru import logger
import uuid
import json
import os

from .schemas import (
    PipelineData,
    PlayerSchema,
    MatchSchema,
    EventSchema,
    CompetitionSchema,
    RankingSchema,
    ValidationResult,
    ValidationError,
    ValidationSeverity,
)
from .validators import TechnicalValidator, BusinessValidator
from .events import EventPublisher, EventType, DataChangeEvent


class DataPipeline:
    """
    4ë‹¨ê³„ ë°ì´í„° íŒŒì´í”„ë¼ì¸
    
    Stage 1: Raw Storage (ì›ë³¸ ì €ìž¥)
    Stage 2: Technical Validation (ê¸°ìˆ ì  ê²€ì¦)
    Stage 3: Business Logic Validation (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê²€ì¦)
    Stage 4: Validated Storage (ê²€ì¦ëœ ë°ì´í„° ì €ìž¥)
    """
    
    def __init__(self, db_client=None, raw_storage_path: str = "data/raw"):
        self.db = db_client
        self.raw_storage_path = raw_storage_path
        
        # ê²€ì¦ê¸° ì´ˆê¸°í™”
        self.tech_validator = TechnicalValidator(db_client)
        self.biz_validator = BusinessValidator(db_client)
        
        # ì´ë²¤íŠ¸ ë°œí–‰ìž ì´ˆê¸°í™”
        self.publisher = EventPublisher(db_client)
        
        # í†µê³„
        self.stats = {
            "total_processed": 0,
            "total_passed": 0,
            "total_failed": 0,
            "total_warnings": 0,
        }
        
        # ì›ë³¸ ì €ìž¥ì†Œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(raw_storage_path, exist_ok=True)
    
    # ==================== Stage 1: Raw Storage ====================
    
    def save_raw_data(
        self,
        source_url: str,
        content: str,
        content_type: str = "html"
    ) -> str:
        """
        ì›ë³¸ ë°ì´í„° ì €ìž¥ (ê²€ì¦ ì—†ì´)
        
        Args:
            source_url: ë°ì´í„° ì¶œì²˜ URL
            content: ì›ë³¸ ì½˜í…ì¸  (HTML ë˜ëŠ” JSON)
            content_type: ì½˜í…ì¸  íƒ€ìž… ("html" ë˜ëŠ” "json")
        
        Returns:
            scrape_id: ìŠ¤í¬ëž˜í•‘ ê³ ìœ  ID
        """
        scrape_id = str(uuid.uuid4())
        timestamp = datetime.now()
        
        # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        date_dir = os.path.join(
            self.raw_storage_path,
            timestamp.strftime("%Y/%m/%d")
        )
        os.makedirs(date_dir, exist_ok=True)
        
        # ë©”íƒ€ë°ì´í„°
        metadata = {
            "scrape_id": scrape_id,
            "source_url": source_url,
            "content_type": content_type,
            "scraped_at": timestamp.isoformat(),
            "content_length": len(content),
        }
        
        # íŒŒì¼ ì €ìž¥
        content_file = os.path.join(date_dir, f"{scrape_id}.{content_type}")
        meta_file = os.path.join(date_dir, f"{scrape_id}.meta.json")
        
        with open(content_file, "w", encoding="utf-8") as f:
            f.write(content)
        
        with open(meta_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ðŸ“¦ Raw data saved: {scrape_id}")
        
        return scrape_id
    
    def load_raw_data(self, scrape_id: str) -> Optional[Tuple[str, Dict[str, Any]]]:
        """
        ì €ìž¥ëœ ì›ë³¸ ë°ì´í„° ë¡œë“œ
        
        Returns:
            (content, metadata) ë˜ëŠ” None
        """
        # ëª¨ë“  ë‚ ì§œ ë””ë ‰í† ë¦¬ ê²€ìƒ‰
        for root, dirs, files in os.walk(self.raw_storage_path):
            meta_file = os.path.join(root, f"{scrape_id}.meta.json")
            if os.path.exists(meta_file):
                with open(meta_file, "r", encoding="utf-8") as f:
                    metadata = json.load(f)
                
                content_type = metadata.get("content_type", "html")
                content_file = os.path.join(root, f"{scrape_id}.{content_type}")
                
                if os.path.exists(content_file):
                    with open(content_file, "r", encoding="utf-8") as f:
                        content = f.read()
                    return content, metadata
        
        return None
    
    # ==================== Stage 2 & 3: Validation ====================
    
    def validate_data(
        self,
        data: Dict[str, Any],
        data_type: str,
        existing_data: Optional[Dict[str, Any]] = None
    ) -> Tuple[ValidationResult, ValidationResult]:
        """
        ë°ì´í„° ê²€ì¦ (ê¸°ìˆ ì  + ë¹„ì¦ˆë‹ˆìŠ¤)
        
        Args:
            data: ê²€ì¦í•  ë°ì´í„°
            data_type: ë°ì´í„° íƒ€ìž… ("player", "match", "event", "competition", "ranking")
            existing_data: ê¸°ì¡´ ë°ì´í„° (ì—…ë°ì´íŠ¸ ì‹œ)
        
        Returns:
            (ê¸°ìˆ ì  ê²€ì¦ ê²°ê³¼, ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ì¦ ê²°ê³¼)
        """
        # Stage 2: ê¸°ìˆ ì  ê²€ì¦
        if data_type == "player":
            tech_result = self.tech_validator.validate_player(data)
        elif data_type == "match":
            tech_result = self.tech_validator.validate_match(data)
        elif data_type == "event":
            tech_result = self.tech_validator.validate_event(data)
        elif data_type == "competition":
            tech_result = self.tech_validator.validate_competition(data)
        elif data_type == "ranking":
            tech_result = self.tech_validator.validate_ranking(data)
        else:
            tech_result = ValidationResult(
                is_valid=False,
                errors=[ValidationError(
                    error_type="UNKNOWN_DATA_TYPE",
                    severity=ValidationSeverity.CRITICAL,
                    message=f"ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° íƒ€ìž…: {data_type}"
                )]
            )
        
        # ê¸°ìˆ ì  ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ì¦ ê±´ë„ˆë›°ê¸°
        if not tech_result.can_save:
            return tech_result, ValidationResult(is_valid=False)
        
        # Stage 3: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê²€ì¦
        biz_result = self.biz_validator.validate_full(data, data_type, existing_data)
        
        return tech_result, biz_result
    
    # ==================== Stage 4: Validated Storage ====================
    
    def save_validated_data(
        self,
        data: Dict[str, Any],
        data_type: str,
        tech_result: ValidationResult,
        biz_result: ValidationResult
    ) -> Optional[int]:
        """
        ê²€ì¦ëœ ë°ì´í„° ì €ìž¥
        
        Returns:
            ì €ìž¥ëœ ë ˆì½”ë“œ ID ë˜ëŠ” None
        """
        if not self.db:
            logger.error("DB í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        # ê²€ì¦ í†µê³¼ í™•ì¸
        if not tech_result.can_save:
            logger.warning(f"ê¸°ìˆ ì  ê²€ì¦ ì‹¤íŒ¨: {data_type}")
            self._log_validation_failure(data, data_type, tech_result)
            return None
        
        if not biz_result.can_save:
            logger.warning(f"ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ì¦ ì‹¤íŒ¨: {data_type}")
            self._log_validation_failure(data, data_type, biz_result)
            return None
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        data["validated_at"] = datetime.now().isoformat()
        data["validation_version"] = "1.0"
        
        if biz_result.warnings:
            data["has_warnings"] = True
            data["warnings"] = [w.message for w in biz_result.warnings]
        
        try:
            # í…Œì´ë¸” ì´ë¦„ ê²°ì •
            table_map = {
                "player": "players",
                "match": "matches",
                "event": "events",
                "competition": "competitions",
                "ranking": "rankings",
            }
            table_name = table_map.get(data_type)
            
            if not table_name:
                logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” í…Œì´ë¸”: {data_type}")
                return None
            
            # Upsert ì‹¤í–‰
            result = self.db.table(table_name).upsert(data).execute()
            
            if result.data:
                record_id = result.data[0].get("id")
                
                # ì´ë²¤íŠ¸ ë°œí–‰
                event_type_map = {
                    "player": EventType.PLAYER_CREATED,
                    "match": EventType.MATCH_CREATED,
                    "event": EventType.EVENT_CREATED,
                    "competition": EventType.COMPETITION_CREATED,
                }
                
                event_type = event_type_map.get(data_type)
                if event_type:
                    self.publisher.publish(DataChangeEvent(
                        event_type=event_type,
                        entity_type=data_type,
                        entity_id=record_id,
                        data=data
                    ))
                
                self.stats["total_passed"] += 1
                logger.info(f"âœ… Data saved: {data_type}#{record_id}")
                return record_id
            
            return None
        
        except Exception as e:
            logger.error(f"ë°ì´í„° ì €ìž¥ ì‹¤íŒ¨: {e}")
            self.stats["total_failed"] += 1
            return None
    
    def _log_validation_failure(
        self,
        data: Dict[str, Any],
        data_type: str,
        result: ValidationResult
    ) -> None:
        """ê²€ì¦ ì‹¤íŒ¨ ë¡œê¹…"""
        self.stats["total_failed"] += 1
        
        # ê²€ì¦ ì‹¤íŒ¨ ì´ë²¤íŠ¸ ë°œí–‰
        self.publisher.publish(DataChangeEvent(
            event_type=EventType.VALIDATION_FAILED,
            entity_type=data_type,
            data={
                "original_data": data,
                "errors": [e.message for e in result.errors],
            }
        ))
        
        # DBì— ì‹¤íŒ¨ ë¡œê·¸ ì €ìž¥
        if self.db:
            try:
                self.db.table("validation_logs").insert({
                    "data_type": data_type,
                    "data": data,
                    "errors": [e.to_dict() if hasattr(e, 'to_dict') else {"message": e.message, "severity": e.severity.value} for e in result.errors],
                    "warnings": [w.to_dict() if hasattr(w, 'to_dict') else {"message": w.message, "severity": w.severity.value} for w in result.warnings],
                    "created_at": datetime.now().isoformat()
                }).execute()
            except Exception as e:
                logger.warning(f"ê²€ì¦ ë¡œê·¸ ì €ìž¥ ì‹¤íŒ¨: {e}")
    
    # ==================== ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ====================
    
    def process(
        self,
        data: Dict[str, Any],
        data_type: str,
        source_url: str = "",
        existing_data: Optional[Dict[str, Any]] = None
    ) -> Tuple[Optional[int], ValidationResult, ValidationResult]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            data: ì²˜ë¦¬í•  ë°ì´í„°
            data_type: ë°ì´í„° íƒ€ìž…
            source_url: ë°ì´í„° ì¶œì²˜ URL
            existing_data: ê¸°ì¡´ ë°ì´í„° (ì—…ë°ì´íŠ¸ ì‹œ)
        
        Returns:
            (ì €ìž¥ëœ ID, ê¸°ìˆ ì  ê²€ì¦ ê²°ê³¼, ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ì¦ ê²°ê³¼)
        """
        self.stats["total_processed"] += 1
        
        # Stage 1: ì›ë³¸ ì €ìž¥ (ì„ íƒì )
        if source_url:
            self.save_raw_data(source_url, json.dumps(data), "json")
        
        # Stage 2 & 3: ê²€ì¦
        tech_result, biz_result = self.validate_data(data, data_type, existing_data)
        
        # Stage 4: ì €ìž¥
        record_id = None
        if tech_result.can_save and biz_result.can_save:
            record_id = self.save_validated_data(data, data_type, tech_result, biz_result)
        
        # ê²½ê³  ì¹´ìš´íŠ¸
        self.stats["total_warnings"] += len(tech_result.warnings) + len(biz_result.warnings)
        
        return record_id, tech_result, biz_result
    
    def process_batch(
        self,
        records: List[Dict[str, Any]],
        data_type: str,
        source_url: str = ""
    ) -> Dict[str, Any]:
        """
        ë°°ì¹˜ ì²˜ë¦¬
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        results = {
            "total": len(records),
            "success": 0,
            "failed": 0,
            "warnings": 0,
            "errors": [],
        }
        
        for i, record in enumerate(records):
            try:
                record_id, tech_result, biz_result = self.process(
                    record, data_type, source_url
                )
                
                if record_id:
                    results["success"] += 1
                else:
                    results["failed"] += 1
                    # ì˜¤ë¥˜ ìˆ˜ì§‘
                    for error in tech_result.errors + biz_result.errors:
                        results["errors"].append({
                            "record_index": i,
                            "error": error.message,
                            "severity": error.severity.value
                        })
                
                results["warnings"] += len(tech_result.warnings) + len(biz_result.warnings)
            
            except Exception as e:
                results["failed"] += 1
                results["errors"].append({
                    "record_index": i,
                    "error": str(e),
                    "severity": "critical"
                })
        
        logger.info(f"ðŸ“Š Batch processing completed: {results['success']}/{results['total']} success")
        
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ í†µê³„ ì¡°íšŒ"""
        return {
            **self.stats,
            "pass_rate": (
                self.stats["total_passed"] / self.stats["total_processed"]
                if self.stats["total_processed"] > 0 else 0
            )
        }
    
    def reset_stats(self) -> None:
        """í†µê³„ ì´ˆê¸°í™”"""
        self.stats = {
            "total_processed": 0,
            "total_passed": 0,
            "total_failed": 0,
            "total_warnings": 0,
        }
