#!/usr/bin/env python3
"""
DE (Direct Elimination) ë°ì´í„° ë³´ì™„ ìŠ¤í¬ë˜í¼

ê¸°ì¡´ JSON ë°ì´í„°ì—ì„œ DE ë°ì´í„°ê°€ ëˆ„ë½ëœ ì¢…ëª©ë§Œ ì¬ìˆ˜ì§‘
"""

import asyncio
import json
import argparse
from datetime import datetime
from pathlib import Path
from loguru import logger

# ê¸°ì¡´ ìŠ¤í¬ë˜í¼ import
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))
from scraper.full_scraper import KFFFullScraper, throttle_request


async def rescrape_de_only(
    input_file: str = "data/fencing_full_data_v2.json",
    output_file: str = None,
    limit: int = None,
    start_idx: int = 0
):
    """
    DE ë°ì´í„°ë§Œ ë³´ì™„ ìŠ¤í¬ë˜í•‘

    Args:
        input_file: ê¸°ì¡´ ë°ì´í„° íŒŒì¼
        output_file: ì¶œë ¥ íŒŒì¼ (Noneì´ë©´ ì…ë ¥ íŒŒì¼ ë®ì–´ì“°ê¸°)
        limit: ì²˜ë¦¬í•  ëŒ€íšŒ ìˆ˜ ì œí•œ
        start_idx: ì‹œì‘í•  ëŒ€íšŒ ì¸ë±ìŠ¤ (0-based)
    """
    if output_file is None:
        output_file = input_file

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    logger.info(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    competitions = data.get("competitions", [])
    total = len(competitions)

    # DE ë°ì´í„° ëˆ„ë½ëœ ëŒ€íšŒ ì°¾ê¸°
    need_de = []
    for i, comp in enumerate(competitions):
        events = comp.get("events", [])
        if not events:
            continue  # ì¢…ëª© ë°ì´í„° ì—†ëŠ” ëŒ€íšŒëŠ” ì „ì²´ ì¬ìŠ¤í¬ë˜í•‘ í•„ìš”

        # DE ë°ì´í„° í™•ì¸
        has_de = False
        for event in events:
            de_matches = event.get("de_matches", [])
            match_results = event.get("de_bracket", {}).get("match_results", [])
            if de_matches or match_results:
                has_de = True
                break

        if not has_de:
            need_de.append(i)

    logger.info(f"DE ë°ì´í„° ëˆ„ë½ ëŒ€íšŒ: {len(need_de)}ê°œ / ì „ì²´ {total}ê°œ")

    # ì‹œì‘ ì¸ë±ìŠ¤ ì ìš©
    need_de = [i for i in need_de if i >= start_idx]

    # ì œí•œ ì ìš©
    if limit:
        need_de = need_de[:limit]

    logger.info(f"ì²˜ë¦¬ ëŒ€ìƒ: {len(need_de)}ê°œ ëŒ€íšŒ (ì‹œì‘: {start_idx})")

    if not need_de:
        logger.info("ì²˜ë¦¬í•  ëŒ€íšŒê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    async with KFFFullScraper(headless=True) as scraper:
        for idx, comp_idx in enumerate(need_de):
            comp = competitions[comp_idx]
            comp_info = comp.get("competition", {})
            comp_name = comp_info.get("name", "Unknown")
            event_cd = comp_info.get("event_cd", "")

            logger.info(f"[{idx+1}/{len(need_de)}] [{comp_idx+1}] {comp_name}")

            events = comp.get("events", [])
            updated = 0

            # í˜ì´ì§€ ë²ˆí˜¸ ê³„ì‚° (ëŒ€íšŒ ëª©ë¡ì€ 10ê°œì”© í˜ì´ì§€ë„¤ì´ì…˜)
            page_num = (comp_idx // 10) + 1

            for event in events:
                sub_event_cd = event.get("sub_event_cd", "")
                event_name = event.get("name", "Unknown")

                try:
                    # DE ë°ì´í„°ë§Œ ìˆ˜ì§‘
                    de_data = await scraper.get_de_only(event_cd, sub_event_cd, page_num=page_num)

                    if de_data.get("de_matches") or de_data.get("de_bracket", {}).get("match_results"):
                        event["de_bracket"] = de_data.get("de_bracket", {})
                        event["de_matches"] = de_data.get("de_matches", [])
                        updated += 1
                        logger.debug(f"  âœ… {event_name}: DE {len(de_data.get('de_matches', []))}ê°œ")

                except Exception as e:
                    logger.error(f"  âŒ {event_name}: {e}")

                await throttle_request()

            logger.info(f"  ì—…ë°ì´íŠ¸: {updated}/{len(events)} ì¢…ëª©")

            # ì¤‘ê°„ ì €ì¥
            if (idx + 1) % 3 == 0:
                data["meta"]["updated_at"] = datetime.now().isoformat()
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2, default=str)
                logger.info(f"  ğŸ’¾ ì¤‘ê°„ ì €ì¥: {output_file}")

    # ìµœì¢… ì €ì¥
    data["meta"]["updated_at"] = datetime.now().isoformat()
    data["meta"]["de_rescrape_completed"] = datetime.now().isoformat()

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2, default=str)

    logger.info(f"âœ… DE ë³´ì™„ ì™„ë£Œ: {output_file}")


async def rescrape_full_missing(
    input_file: str = "data/fencing_full_data_v2.json",
    output_file: str = None,
    limit: int = None
):
    """
    ì¢…ëª© ë°ì´í„°ê°€ ì—†ëŠ” ëŒ€íšŒ ì „ì²´ ì¬ìŠ¤í¬ë˜í•‘
    """
    if output_file is None:
        output_file = input_file

    logger.info(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    competitions = data.get("competitions", [])

    # ì¢…ëª© ë°ì´í„° ì—†ëŠ” ëŒ€íšŒ ì°¾ê¸°
    need_full = []
    for i, comp in enumerate(competitions):
        events = comp.get("events", [])
        if not events:
            need_full.append(i)

    logger.info(f"ì¢…ëª© ë°ì´í„° ëˆ„ë½ ëŒ€íšŒ: {len(need_full)}ê°œ")

    if limit:
        need_full = need_full[:limit]

    if not need_full:
        logger.info("ì²˜ë¦¬í•  ëŒ€íšŒê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    async with KFFFullScraper(headless=True) as scraper:
        for idx, comp_idx in enumerate(need_full):
            comp = competitions[comp_idx]
            comp_info = comp.get("competition", {})

            # Competition ê°ì²´ ì¬êµ¬ì„±
            from scraper.full_scraper import Competition
            comp_obj = Competition(
                event_cd=comp_info.get("event_cd", ""),
                name=comp_info.get("name", ""),
                start_date=comp_info.get("start_date", ""),
                end_date=comp_info.get("end_date", ""),
                location=comp_info.get("location", ""),
                host=comp_info.get("host", ""),
                status=comp_info.get("status", "ì¢…ë£Œ")
            )

            logger.info(f"[{idx+1}/{len(need_full)}] [{comp_idx+1}] {comp_obj.name}")

            try:
                # ì „ì²´ ë°ì´í„° ì¬ìˆ˜ì§‘
                page_num = (comp_idx // 10) + 1
                comp_data = await scraper.scrape_competition_full(comp_obj, page_num=page_num)

                # ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸
                competitions[comp_idx] = comp_data

                logger.info(f"  âœ… {len(comp_data.get('events', []))}ê°œ ì¢…ëª© ìˆ˜ì§‘")

            except Exception as e:
                logger.error(f"  âŒ ì‹¤íŒ¨: {e}")

            # ì¤‘ê°„ ì €ì¥
            if (idx + 1) % 3 == 0:
                data["meta"]["updated_at"] = datetime.now().isoformat()
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2, default=str)
                logger.info(f"  ğŸ’¾ ì¤‘ê°„ ì €ì¥")

            await throttle_request()

    # ìµœì¢… ì €ì¥
    data["meta"]["updated_at"] = datetime.now().isoformat()

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2, default=str)

    logger.info(f"âœ… ì „ì²´ ì¬ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {output_file}")


async def main():
    parser = argparse.ArgumentParser(description="DE ë°ì´í„° ë³´ì™„ ìŠ¤í¬ë˜í¼")
    parser.add_argument("--mode", choices=["de", "full", "both"], default="de",
                       help="de: DEë§Œ ë³´ì™„, full: ëˆ„ë½ëœ ëŒ€íšŒ ì „ì²´ ì¬ìˆ˜ì§‘, both: ë‘˜ ë‹¤")
    parser.add_argument("--input", type=str, default="data/fencing_full_data_v2.json",
                       help="ì…ë ¥ íŒŒì¼")
    parser.add_argument("--output", type=str, default=None,
                       help="ì¶œë ¥ íŒŒì¼ (ê¸°ë³¸: ì…ë ¥ íŒŒì¼ ë®ì–´ì“°ê¸°)")
    parser.add_argument("--limit", type=int, default=None,
                       help="ì²˜ë¦¬í•  ëŒ€íšŒ ìˆ˜ ì œí•œ")
    parser.add_argument("--start", type=int, default=0,
                       help="ì‹œì‘í•  ëŒ€íšŒ ì¸ë±ìŠ¤ (0-based)")

    args = parser.parse_args()

    if args.mode in ["de", "both"]:
        await rescrape_de_only(
            input_file=args.input,
            output_file=args.output,
            limit=args.limit,
            start_idx=args.start
        )

    if args.mode in ["full", "both"]:
        await rescrape_full_missing(
            input_file=args.input,
            output_file=args.output,
            limit=args.limit
        )


if __name__ == "__main__":
    asyncio.run(main())
